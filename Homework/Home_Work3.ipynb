{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ec340a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated A12 and A21: tensor([1.9584, 1.6893], requires_grad=True)\n",
      "Objective Value: tensor(0.6702, grad_fn=<AddBackward0>)\n",
      "calculated value 28.8\n",
      "calculated value 34.6\n",
      "calculated value 36.5\n",
      "calculated value 36.9\n",
      "calculated value 36.9\n",
      "calculated value 36.8\n",
      "calculated value 36.4\n",
      "calculated value 35.4\n",
      "calculated value 32.9\n",
      "calculated value 27.7\n",
      "calculated value 17.5\n",
      "Given Values of p [28.1, 34.4, 36.7, 36.9, 36.8, 36.7, 36.5, 35.4, 32.9, 27.7, 17.5]\n",
      "By camparing these two Calculated values and given values, the output: Calculated data is close to the given values of data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch as t\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#Feeding the given data(in problem) as some variables\n",
    "x1_info = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0] \n",
    "p_info = [28.1,34.4,36.7,36.9,36.8,36.7,36.5,35.4,32.9,27.7,17.5]\n",
    "p_satw = 10.0**(8.07131-1730.63/(20.0+233.426))\n",
    "p_satd = 10.0**(7.43155-1554.679/(20.0+240.337))\n",
    "\n",
    "def loss(a): #defining the loss function\n",
    "    total_loss= 0.0\n",
    "    for i in range (11):\n",
    "        x1 = x1_info[i]\n",
    "        p= p_info[i]\n",
    "        x2= 1-x1\n",
    "        p_norm = x1*p_satw*t.exp(a[0]*(a[1]*(x2)/(a[0]*x1+a[1]*(x2)))**2) + (x2)*p_satd*t.exp(a[1]*(a[0]*x1/(a[0]*x1+a[1]*(x2)))**2)\n",
    "        total_loss = total_loss + (p_norm-p)**2\n",
    "    return total_loss #returning the defined loss value \n",
    "\n",
    "alpha = 1 #giving the error value as aplha\n",
    "A= Variable(t.tensor ([1.0, 1.0]), requires_grad=True)\n",
    "\n",
    "\n",
    "while alpha >= 0.05:\n",
    "        loss(A).backward()\n",
    "        alpha = t.linalg.norm(A.grad)\n",
    "        X=.2\n",
    "        while loss(A-X*A.grad)>loss(A):\n",
    "            X=.5*X\n",
    "        with t.no_grad():\n",
    "            A-= X*A.grad\n",
    "            A.grad.zero_()\n",
    "print ('Estimated A12 and A21:',A)\n",
    "print ('Objective Value:',loss(A))\n",
    "from math import exp\n",
    "\n",
    "for i in range (11) :\n",
    "    x1 = x1_info[i]\n",
    "    x2= 1-x1\n",
    "    p_norm = x1*p_satw*exp(A[0]*(A[1]*(x2)/(A[0]*x1+A[1]*(x2)))**2) + (x2)*p_satd*exp(A[1]*(A[0]*x1/(A[0]*x1+A[1]*(x2)))**2)\n",
    "    print ('calculated value', round(p_norm,1))\n",
    "print('Given Values of p',p_info)\n",
    "print('By camparing these two Calculated values and given values, the output: Calculated data is close to the given values of data')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf57ac7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_sigma' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-63a3970f0685>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdivide\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0my_best\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_sigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_best\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_sigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mei\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_mu\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my_best\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mcdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mY_sigma\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Y_sigma' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.gaussian_process as gp\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def objective (X, mean = 0, var = 0.1):\n",
    "    noise = np.random.normal(mean, var)\n",
    "    return ((4 - (2.1* (X[0]**2)) + ((X[0]**4) /3))* (X[0]**2)) + (X[0]*X[1]) + ((-4 + 4*x[1]**2))\n",
    "\n",
    "def imp(X_, X, model, k = 0.01):\n",
    "    Y_mu, Y_sigma = model.predict(X, return_std = True)\n",
    "    Y__= model.predict(X_)\n",
    "    \n",
    "with np.errstate (divide='ignore'):\n",
    "    y_best = np.max(Y_sigma)\n",
    "    Z = (Y_mu - y_best - k)/(Y_sigma)\n",
    "    ei= ((Y_mu - y_best - k)*norm. cdf(Z)) + (Y_sigma*norm.pdf(Z))\n",
    "    ei[Y_sigma == 0.0]= 0.0\n",
    "return X_[np.argmax(ei)]\n",
    "   \n",
    "x1= np.random.uniform(-3,3,5)\n",
    "x2= np.random.uniform(-2,2,5)\n",
    "X= p.asarray([[x1], [x2]]).transpose().squeeze(1)\n",
    "Y= np. asarray ([objective(x) for x in X])\n",
    "       \n",
    "kernel = gp.kernels.RBF()\n",
    "model = gp. GaussianProcessRegressor (kernel=kernel,\n",
    "                                      alpha=1e-4,\n",
    "                                      n_restarts_optimizer=0,\n",
    "                                      normalize_y=True)\n",
    "\n",
    "n_iters = 10\n",
    "k = 0.01\n",
    "for i in range (n_iters):\n",
    "       model.fit(x,Y)\n",
    "       # Select next point using expected improvement\n",
    "       x1 = mp.random.uniform(-3,3,5)\n",
    "       x2 = np.random.uniform(-2,2,5)\n",
    "       X= np. asarray ([[x1],[x2]]). transpose ().squeeze (1)\n",
    "       x_next = imp(X_= X_,X = X, model = model)\n",
    "       y_next = objective (x_next)\n",
    "       X.stack (x_next)\n",
    "       Y.append (y_next)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42202dd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1d0dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
